\# Scaling Laws and the Transformer architecture



This is my implementation of two very famous papers in the field of DL and NLP.

* Attention Is All You Need https://arxiv.org/pdf/1706.03762
* Scaling Laws for Neural Language Models https://arxiv.org/pdf/2001.08361
  

In this notebook you will find:

* Transformer architecture, with self attention, cross attention, positional encoding handwritten and self implemented in pytorch
* Training the architecture for machine translation task on wmt14 de-en dataset (through hugging face).
* Implementation of two scaling laws mentioned in the Scaling Laws for Neural Language Models paper

&nbsp;	1. Performance with Non-Embedding Parameter Count N follows a power law

&nbsp;	2. Performance with Dataset Size D follows a power law

